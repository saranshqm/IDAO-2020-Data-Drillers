{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "idao better file.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN2f8UhWBNlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import xgboost\n",
        "import csv as csv\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import cross_val_score,KFold\n",
        "#from sklearn.cross_validation import  train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "#from sklearn.grid_search import GridSearchCV2 #Perforing grid search\n",
        "from scipy.stats import skew\n",
        "from collections import OrderedDict\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGpPJGlCBnW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "174edb62-af8c-4940-ee75-8d552995ee54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBUeP-xYBtfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krNJUMvmBNlq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2c6a224-8b63-454c-ff44-a2705700d91b"
      },
      "source": [
        "#loading train and test data-set\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/My Drive/IDAO 2020/IDAO 2020/train.csv\",header=0)\n",
        "test = pd.read_csv(\"/content/drive/My Drive/IDAO 2020/IDAO 2020/Track 1/test.csv\",header=0)\n",
        "print(train.shape,test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(649912, 15) (284071, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkytJ33pBNlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "fca84643-ac31-4934-9def-94ba9bc9aadf"
      },
      "source": [
        "\n",
        "\n",
        "def date_time_extract(a,count):\n",
        "    if count == 0:\n",
        "        print('extracting date, time, hour......for train data')\n",
        "        train['hour'] = pd.DatetimeIndex(a).hour\n",
        "        train['minute'] = pd.DatetimeIndex(a).minute\n",
        "        train['second'] = pd.DatetimeIndex(a).second\n",
        "        train['day'] = pd.DatetimeIndex(a).day\n",
        "        print('extraction of date_time_hour done.........!!')\n",
        "    else:\n",
        "        print('extracting date, time, hour......for test data')\n",
        "        test['hour'] = pd.DatetimeIndex(a).hour\n",
        "        test['minute'] = pd.DatetimeIndex(a).minute\n",
        "        test['second'] = pd.DatetimeIndex(a).second\n",
        "        test['day'] = pd.DatetimeIndex(a).day\n",
        "        print('extraction of date_time_hour done.........!!')\n",
        "    \n",
        "    count+=1\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "date_time_extract(train['epoch'],0)\n",
        "date_time_extract(test['epoch'],1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "extracting date, time, hour......for train data\n",
            "extraction of date_time_hour done.........!!\n",
            "extracting date, time, hour......for test data\n",
            "extraction of date_time_hour done.........!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNYBekSdBNly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_acceleration(a,b,c,count):\n",
        "    a1 = []\n",
        "    a2 = []\n",
        "    a3 = []\n",
        "    \n",
        "    #a1.append(0)\n",
        "    #a2.append(0)\n",
        "    #a3.append(0)\n",
        "    \n",
        "    a1 = np.asarray(a)\n",
        "    a2 = np.asarray(b)\n",
        "    a3 = np.asarray(c)\n",
        "    \n",
        "    b1 = []\n",
        "    b2 = []\n",
        "    b3 = []\n",
        "    \n",
        "    if count == 0:\n",
        "    \n",
        "        for i in range(len(np.asarray(train['hour']))-1):\n",
        "            #print('extracting acceleration in 3-D....')\n",
        "            b1.append((a1[i+1]-a1[i]))\n",
        "            b2.append((a2[i+1]-a2[i]))\n",
        "            b3.append((a3[i+1]-a3[i]))\n",
        "        \n",
        "        b1.append(a[len(a)-1])\n",
        "        b2.append(b[len(a)-1])\n",
        "        b3.append(c[len(a)-1])\n",
        "    \n",
        "        print('extraction done!!')\n",
        "    \n",
        "        print('loading acceleration values in train...')\n",
        "        train['a_x'] = np.asarray(b1)\n",
        "        train['a_y'] = np.asarray(b2)\n",
        "        train['a_z'] = np.asarray(b3)\n",
        "        print('loaded acc values in train...')\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        for i in range(len(np.asarray(test['hour']))-1):\n",
        "            #print('extracting acceleration in 3-D....')\n",
        "            b1.append((a1[i+1]-a1[i]))\n",
        "            b2.append((a2[i+1]-a2[i]))\n",
        "            b3.append((a3[i+1]-a3[i]))\n",
        "        \n",
        "        b1.append(a[len(a)-1])\n",
        "        b2.append(b[len(a)-1])\n",
        "        b3.append(c[len(a)-1])\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        print('loading acceleration values in test')\n",
        "        test['a_x'] = np.asarray(b1)\n",
        "        test['a_y'] = np.asarray(b2)\n",
        "        test['a_z'] = np.asarray(b3)\n",
        "        print('loaded acc values in test')\n",
        "    \n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwuO0-w1BNl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e5030d6a-39b1-41a8-8cde-57c513493669"
      },
      "source": [
        "get_acceleration(train['Vx_sim'],train['Vy_sim'],train['Vz_sim'],0)\n",
        "get_acceleration(test['Vx_sim'],test['Vy_sim'],test['Vz_sim'],1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "extraction done!!\n",
            "loading acceleration values in train...\n",
            "loaded acc values in train...\n",
            "loading acceleration values in test\n",
            "loaded acc values in test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZO2kQn-BNl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(t,count):\n",
        "    if count == 0:\n",
        "        feature_df_train = train[['Vx_sim', 'Vy_sim', 'Vz_sim', 'x_sim', 'y_sim', 'z_sim','a_x','a_y','a_z']]\n",
        "        return feature_df_train\n",
        "    \n",
        "    else:\n",
        "        feature_df_test = test[['Vx_sim', 'Vy_sim', 'Vz_sim', 'x_sim', 'y_sim', 'z_sim','a_x','a_y','a_z']]\n",
        "        return feature_df_test\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmKMEaT5BNl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = get_features(train,0)\n",
        "x_train = np.asarray(x_train)\n",
        "x_test = get_features(test,0)\n",
        "x_test = np.asarray(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJiAeR7jBNl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_target_variable(x):\n",
        "    return np.asarray(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVV17FzrBNmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_x = get_target_variable(train['x'])\n",
        "y_y = get_target_variable(train['y'])\n",
        "y_z = get_target_variable(train['z'])\n",
        "\n",
        "y_vx = get_target_variable(train['Vx'])\n",
        "y_vy = get_target_variable(train['Vy'])\n",
        "y_vz = get_target_variable(train['Vz'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFym96ChBNmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLpPYjMtBNmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_split,x_val_split,y_train_x,y_val_x,y_train_y,y_val_y,y_train_z,y_val_z,y_train_vx, y_val_vx,y_train_vy,y_val_vy, y_train_vz,y_val_vz = train_test_split(x_train,y_x,y_y,y_z,y_vx,y_vy,y_vz,test_size = 0.3,random_state = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ei-52pGN1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oEWuhp5GNzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "01eaa7a5-5a40-4ff2-95f8-5cc4e4aa2563"
      },
      "source": [
        "#using neural network\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = x_train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               1280      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,381,121\n",
            "Trainable params: 1,381,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu8Qyd2XGNwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZIxvH8OGkFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10bd6516-aa7b-484a-ded8-1ed8ff608d64"
      },
      "source": [
        "NN_model.fit(x_train, y_x, epochs=30, batch_size=128, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 519929 samples, validate on 129983 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "519929/519929 [==============================] - 165s 318us/step - loss: 3221.0762 - mean_absolute_error: 3221.0762 - val_loss: 3917.0329 - val_mean_absolute_error: 3917.0329\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3917.03286, saving model to Weights-001--3917.03286.hdf5\n",
            "Epoch 2/30\n",
            "519929/519929 [==============================] - 165s 317us/step - loss: 3073.5729 - mean_absolute_error: 3073.5729 - val_loss: 3868.1020 - val_mean_absolute_error: 3868.1020\n",
            "\n",
            "Epoch 00002: val_loss improved from 3917.03286 to 3868.10204, saving model to Weights-002--3868.10204.hdf5\n",
            "Epoch 3/30\n",
            "519929/519929 [==============================] - 164s 316us/step - loss: 3037.4541 - mean_absolute_error: 3037.4541 - val_loss: 3847.7594 - val_mean_absolute_error: 3847.7594\n",
            "\n",
            "Epoch 00003: val_loss improved from 3868.10204 to 3847.75939, saving model to Weights-003--3847.75939.hdf5\n",
            "Epoch 4/30\n",
            "519929/519929 [==============================] - 164s 315us/step - loss: 3025.7278 - mean_absolute_error: 3025.7278 - val_loss: 3865.5337 - val_mean_absolute_error: 3865.5337\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 3847.75939\n",
            "Epoch 5/30\n",
            "519929/519929 [==============================] - 164s 316us/step - loss: 3004.9780 - mean_absolute_error: 3004.9780 - val_loss: 3831.5946 - val_mean_absolute_error: 3831.5946\n",
            "\n",
            "Epoch 00005: val_loss improved from 3847.75939 to 3831.59461, saving model to Weights-005--3831.59461.hdf5\n",
            "Epoch 6/30\n",
            "519929/519929 [==============================] - 165s 316us/step - loss: 3000.1863 - mean_absolute_error: 3000.1863 - val_loss: 3862.6199 - val_mean_absolute_error: 3862.6199\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 3831.59461\n",
            "Epoch 7/30\n",
            "519929/519929 [==============================] - 165s 316us/step - loss: 3001.5590 - mean_absolute_error: 3001.5590 - val_loss: 3833.4754 - val_mean_absolute_error: 3833.4754\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 3831.59461\n",
            "Epoch 8/30\n",
            "519929/519929 [==============================] - 164s 315us/step - loss: 2983.4862 - mean_absolute_error: 2983.4862 - val_loss: 3894.2830 - val_mean_absolute_error: 3894.2830\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 3831.59461\n",
            "Epoch 9/30\n",
            "519929/519929 [==============================] - 165s 317us/step - loss: 2960.7329 - mean_absolute_error: 2960.7329 - val_loss: 3875.9754 - val_mean_absolute_error: 3875.9754\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 3831.59461\n",
            "Epoch 10/30\n",
            "519929/519929 [==============================] - 164s 316us/step - loss: 2943.1447 - mean_absolute_error: 2943.1447 - val_loss: 3864.6335 - val_mean_absolute_error: 3864.6335\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 3831.59461\n",
            "Epoch 11/30\n",
            "519929/519929 [==============================] - 166s 319us/step - loss: 2920.8184 - mean_absolute_error: 2920.8184 - val_loss: 3910.7030 - val_mean_absolute_error: 3910.7030\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 3831.59461\n",
            "Epoch 12/30\n",
            "519929/519929 [==============================] - 166s 319us/step - loss: 2906.3264 - mean_absolute_error: 2906.3264 - val_loss: 3867.2248 - val_mean_absolute_error: 3867.2248\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3831.59461\n",
            "Epoch 13/30\n",
            "124416/519929 [======>.......................] - ETA: 2:00 - loss: 2888.2884 - mean_absolute_error: 2888.2884"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-251e0796463f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t2FVJfDGkgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKA4aJSXGkd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFBsNcTGGkbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eeWGP5GBNmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### defining the score\n",
        "\n",
        "def smape(satellite_predicted_values, satellite_true_values): \n",
        "    # the division, addition and subtraction are pointwise \n",
        "    beta = np.mean(np.abs((satellite_predicted_values - satellite_true_values) \n",
        "        / (np.abs(satellite_predicted_values) + np.abs(satellite_true_values))))\n",
        "    return 100*(1-beta)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Cz1yw6BNmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "# this is best till now\n",
        "#n_estimators = 2000\n",
        "\n",
        "def xgb_model_evaluation(x_train_split,x_val_split,y_train_x,y_val_x):\n",
        "    #n_Estimators = 10000\n",
        "    model = xgboost.XGBRegressor(colsample_bytree=0.4,\n",
        "                 gamma=0,                 \n",
        "                 learning_rate=0.1,\n",
        "                 max_depth=6,\n",
        "                 min_child_weight=1.5,\n",
        "                 n_estimators=10000,                                                                    \n",
        "                 reg_alpha=0.75,\n",
        "                 reg_lambda=0.45,\n",
        "                 subsample=0.6,\n",
        "                 seed=42)\n",
        "    #print('all parameters enabled--->')\n",
        "    print('train for y_train_x !!------>')\n",
        "    \n",
        "    model.fit(x_train_split,y_train_x)\n",
        "    \n",
        "    print('training for y_train_x completed-------->prediction started for y_val_x')\n",
        "    \n",
        "    a = model.predict(x_val_split)\n",
        "    \n",
        "    print('prediction for y_val_x completed------> evaluating result for validation data')\n",
        "    \n",
        "    print('the score for validation obtained is:',smape(a,y_val_x))\n",
        "    \n",
        "    print('evaluating results for train---->')\n",
        "    a = model.predict(x_train_split)\n",
        "    print('the score for train obtained is:',smape(a,y_train_x))\n",
        "    \n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI_kyNoSBNmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xgb model for y_train_x\n",
        "xgb_model_evaluation(x_train_split,x_val_split,y_train_x,y_val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1rMVlkuBNmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xgb model for y_train_y\n",
        "xgb_model_evaluation(x_train_split,x_val_split,y_train_y,y_val_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofboiWukBNmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xgb model for y_train_x\n",
        "xgb_model_evaluation(x_train_split,x_val_split,y_train_z,y_val_z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak0cB2d0BNmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xgb model for y_train_vx\n",
        "xgb_model_evaluation(x_train_split,x_val_split,y_train_vx,y_val_vx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iibKmsQGBNmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xgb model for y_train_vy\n",
        "xgb_model_evaluation(x_train_split,x_val_split,y_train_vy,y_val_vy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-DfnlOUBNmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xgb model for y_train_vz\n",
        "xgb_model_evaluation(x_train_split,x_val_split,y_train_vz,y_val_vz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RytN8X-JBNmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}